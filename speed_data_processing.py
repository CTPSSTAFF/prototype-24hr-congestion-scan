# Script to process INRIX speed-run data downloaded from RITIS.
#
# Input: CSV file containing speed data for one 'route' for any number
#        of 24-hour periods (i.e., days)
#
# Intermediate output: CSV file that is a copy of the input CSV file,
#                      but only contains attributes of interest.
#                      The column names in the output CSV file are given shorter
#                      names for the sake of convenience: tmc, tstamp, speed, cvalue
#
# Final output: One CSV file containing the data for each 24-hour period
#               in the intermediate output. 
#               At least for the time being, it is the user's responsibility
#               to know, a priori, the list of dates in question.
#
# This script assumes that the downloaded data is organized in 10 minute units.
#
# Overall steps:
# 1. Extract only the attributes of interest from the raw data from RITIS.
#    These attributes are: tmc_code, measurement_tstamp, speed, and cvalue.
# 2. Extract one CSV file for each day's worth of data;
#    at least for now, the user is expected to know the list of dates
#
#

import csv

# String containing names of fields in ouput CSV files;
# these are the "attributes of interest", with shorter names
out_csv_header = 'tmc,tstamp,speed,cvalue\n'

##########################################################################
# Step 1 - Generate a CSV file with only the attributes of interest in it.


# Input and output filenames for Step 1
#
base = r's:/_congestion_data/granularity_10min/'

from_fns =  [ base + 'I90_EB_data/i90_eb_missing.csv',
              base + 'I90_WB_data/i90_wb_missing.csv',
              base + 'I93_NB_data/i93_nb_missing.csv',
              base + 'I93_SB_data/i93_sb_missing.csv',
              base + 'I95_NB_data/i95_nb_missing.csv',
              base + 'I95_SB_data/i95_sb_missing.csv',
              base + 'I495_NB_data/i495_nb_missing.csv',
              base + 'I495_SB_data/i495_sb_missing.csv', 
              base + 'US3_NB_data/us3_nb_missing.csv',
              base + 'US3_SB_data/us3_sb_missing.csv',
              base + 'SR2_EB_data/sr2_eb_missing.csv',
              base + 'SR2_WB_data/sr2_wb_missing.csv',
              base + 'SR24_NB_data/sr24_nb_missing.csv',
              base + 'SR24_SB_data/sr24_sb_missing.csv'
]         
to_fns = [ fn.replace('.csv', '_p1.csv') for fn in from_fns] 


# Definition of function implementing Step 1:
# Generate a CSV file with only the attributes of interest in it
#
# Function to read the raw CSV file generated by RITIS,
# and produce an output CSV file containing all the input records,
# but only with the "attributes of interest" in each row.
#
def extract_csv_with_subset_attrs(in_fname, out_fname):
    s = 'Extracting from ' + in_fname + '\n' + '           to ' + out_fname
    print(s)
    global csv_header
    out_f = open(out_fname,'w')
    out_f.write(out_csv_header)
    with open(in_fname, newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            # print(row['tmc_code'], row['measurement_tstamp'], row['speed'], row['cvalue'])
            out_str = row['tmc_code'] + ',' + row['measurement_tstamp'] + ',' + row['speed'] +  ',' + row['cvalue'] + '\n'
            out_f.write(out_str)
        # end_for
    # end_with
    out_f.close()
# end_def


# Perform Step 1 on all input files
#
for inf,outf in zip(from_fns,to_fns):
    s = 'Input ' + inf
    print(s)
    s = 'Output ' + outf
    print(s)
    extract_csv_with_subset_attrs(inf,outf)
# end_for


# Definition of function implementing Step 2: 
# Extract one day's worth of data from the intermediate output
#
# Function to extract the data for one day from the intermediate output,
# and write it to a CSV file whose name has the form: i93_nb_<yyyy>-<mm>-<dd>.csv
#
# The author acknowledges the use of a large sledge hammer here.
#
def extract_data_for(in_fname, out_fname_prefix, date_str):
    global out_csv_header  
    out_fname = out_fname_prefix + date_str + '.csv'
    out_f = open(out_fname, 'w')
    out_f.write(out_csv_header)

    with open(in_fname, newline='') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            tstamp = row['tstamp']
            parts = tstamp.split(' ')
            date_part = parts[0]
            if date_part == date_str:
                out_str = row['tmc'] + ',' + row['tstamp'] + ',' + row['speed'] +  ',' + row['cvalue'] + '\n'
                out_f.write(out_str)
            # end_if
        # end_for
    # end_with
    out_f.close()
    s = 'Extraction of data for ' + date_str + ' completed.'
    print(s)
# end_def

# List of dates in the input CSV file
all_daytz = = [     '2020-03-28',
                    '2020-03-29',
                    '2020-03-30',
                    '2020-03-31',
                    '2020-04-01',
                    '2020-04-02' ]


# Note: the input to step 2 is the output of step 1.
#        
in_fnames = [   base + 'I90_EB_data/i90_eb_missing_p1.csv',
                base + 'I90_WB_data/i90_wb_missing_p1.csv',
                base + 'I93_NB_data/i93_nb_missing_p1.csv',
                base + 'I93_SB_data/i93_sb_missing_p1.csv',
                base + 'I95_NB_data/i95_nb_missing_p1.csv',
                base + 'I95_SB_data/i95_sb_missing_p1.csv',    
                base + 'I495_NB_data/i495_nb_missing_p1.csv',
                base + 'I495_SB_data/i495_sb_missing_p1.csv', 
                base + 'US3_NB_data/us3_nb_missing_p1.csv',                
                base + 'US3_SB_data/us3_sb_missing_p1.csv', 
                base + 'SR2_EB_data/sr2_eb_missing_p1.csv',
                base + 'SR2_WB_data/sr2_wb_missing_p1.csv',
                base + 'SR24_NB_data/sr24_nb_missing_p1.csv',                
                base + 'SR24_SB_data/sr24_sb_missing_p1.csv' ]
                
out_prefixes = [    base + 'I90_EB_data/i90_eb_',
                    base + 'I90_WB_data/i90_wb_',
                    base + 'I93_NB_data/i93_nb_',
                    base + 'I93_SB_data/i93_sb_',
                    base + 'I95_NB_data/i95_nb_',
                    base + 'I95_SB_data/i95_sb_',    
                    base + 'I495_NB_data/i495_nb_',
                    base + 'I495_SB_data/i495_sb_', 
                    base + 'US3_NB_data/us3_nb_',                
                    base + 'US3_SB_data/us3_sb_', 
                    base + 'SR2_EB_data/sr2_eb_',
                    base + 'SR2_WB_data/sr2_wb_',
                    base + 'SR24_NB_data/sr24_nb_',                
                    base + 'SR24_SB_data/sr24_sb_' ]   

def eff(a,b):
	print(a)
	print(b)
    
# Test of 'zip' function
for (phyle, prefix) in zip(in_fnames, out_prefixes):
    eff(phyle, prefix)    
  
# Perform Step 2 on all files generated as the result of running Step 1.  
for (phyle, prefix) in zip(in_fnames, out_prefixes):
    s1 = 'Processing ' + phyle
    print(s1)
    for d in problem_daytz:
        s2 = '   Processing ' + d
        print(s2)
        extract_data_for(phyle, prefix, d)
# end_for